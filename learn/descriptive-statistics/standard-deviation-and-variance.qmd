---
title: Standard deviation and variance
description: Everything you need to know to learn about standard deviation and variance
---

```{r}
#| echo: false
reticulate::use_virtualenv("../../.venv")
```

```{python}
#| echo: false
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def clean_ax(ax):
   ax.set_xlim(2, 18)
   #ax.set_ylim(0, 0.45)
   ax.spines[["left", "top", "right"]].set_visible(False)
   ax.set_yticks([])
   ax.set_xlabel("Height (in cm)")

np.random.seed(1)

red = "#b20e0e"
blue = "#133ecd"
text_params = dict(x=0, y=0.6, alpha=0.7, size=15, weight="bold")
hist_params = dict(alpha=0.5, bins=15)

sample_size = 200
height_lowvar = np.random.normal(loc=10, scale=1, size=sample_size)
height_highvar = np.random.normal(loc=10, scale=3, size=sample_size)
```

<br>

![](../../img/variance-meme.jpg){width=50% fig-align="center"}

## Measuring dispersion

Imagine you want to measure the <span style="color: #b20e0e; font-weight: bold;">height of plants in your garden</span> because you've noticed that they all have very different heights, but you're not sure how different they actually are.

You also want to compare them with <span style="color: #133ecd; font-weight: bold;">your friend's plants</span>, who keeps saying that all of their plants are almost the same height.

To do that, we need to compare their distributions.

::: {.panel-tabset}

### High variance

```{python}
#| echo: false
fig, ax = plt.subplots()
clean_ax(ax)
ax.hist(height_highvar, color=red, **hist_params)
ax.text(s="Your plants", color=red, transform=ax.transAxes, **text_params)
plt.show()
```

### Low variance

```{python}
#| echo: false
fig, ax = plt.subplots()
clean_ax(ax)
ax.hist(height_lowvar, color=blue, **hist_params)
ax.text(s="Your friend's plants", color=blue, transform=ax.transAxes, **text_params)
plt.show()
```

### Combined

```{python}
#| echo: false
fig, ax = plt.subplots()
clean_ax(ax)
ax.hist(height_highvar, color=red, **hist_params)
ax.hist(height_lowvar, color=blue, **hist_params)
plt.show()
```

:::


Those graphs show us that the height of <span style="color: #b20e0e; font-weight: bold;">your plants</span> can vary from around ~2 cm to ~18 cm, while <span style="color: #133ecd; font-weight: bold;">your friend's plants</span> mostly range from ~8 cm to ~12 cm.

The variance and standard deviation are simply ways to __quantify__ this variation with actual values.

## How to calculate

Suppose you have the following measurements for the height of your plants in cm:

`[10, 15, 4, 22, 25]`

First we need to calculate the average:

$$ average = \frac{10 + 15 + 4 + 22 + 25}{5} =  15.2$$

Now that we have the average, we want to calculate the __difference between each plant and the average__.

For example, the first plant has a height of `10`, which gives us: `10 - 15.2 = -5.2`.

We do the same with the others, and we now have a new list of values: `[-5.2, -0.2, -11.2, 6.8, 9.8]`.

As you can see, we have both negative and positive __values here. To ‘correct’ the situation, we take each value and raise it to the **square**. For example, we have `-5.2*-5.2 = 27.04`, and so on. Our new list now looks like this:

`[27.04, 0.04, 125.44, 46.24, 96.04]`

The final step in obtaining the variance is to add up all these values and divide by the number of values (this is basically like calculating an average):

$$ variance = \frac{27.04 + 0.04 + 125.44 + 46.24 + 96.04}{5} =  58.96 $$

And the standard deviation is just the **square root of the variance**:

$$ std = \sqrt{variance} = \sqrt{58.96} \approx 7.68 $$

If we go back to our earlier example about the plants' heights, you would have a <span style="color: #b20e0e; font-weight: bold;">higher variance and standard deviation</span> compared to <span style="color: #133ecd; font-weight: bold;">your friend's</span>.

## How to interpret

It is __impossible to interpret__ the variance itself. This is because we have squared the values, which changes their original unit (for example, it is not very useful to interpret squared centimetres).

The main thing we can say about variance is that the higher it is, the more __calibrated__ our values tend to be. Conversely, a low variance means that the values tend to be fairly close together.

The standard deviation is much the same, but it is expressed in the __same unit as the original values__ (for example, centimetres).

In __practice__, it becomes interesting to interpret when we know the distribution of our values. For example, if it's a normal distribution, we can use the following:

- Around 68% of values lie within one standard deviation of the mean.

- Around 95% of values are within `2` standard deviations.

- Approximately 99.7% of values are within `3` standard deviations.

For example, if the mean is `10` and the standard deviation is `2`, we can say that :

- Approximately 68% of values are between 8 and 12.

- Around 95% of values are between 6` and 14`.

- Approximately 99.7% of values are between 4` and 16`.

## Code examples

::: {.panel-tabset}

### Python

In Python, there are no built-in functions for calculating variance or standard deviation. We therefore need to use the `numpy` package.

It provides the functions `np.var()` and `np.std()`.

```{python}
import numpy as np

heights = [10, 15, 4, 22, 25]

np.var(heights)
np.std(heights)
```



### R

With R, we can simply use the `var()` and `sd()` functions.

```{r}
heights <- c(10, 15, 4, 22, 25)

var(heights)
sd(heights)
```

:::

Did you notice that R and Python don't give the same values here? Isn't that strange? Well, it's not a bug and it's because they don't calculate exactly the same thing.

In Python, numpy calculates the **variance of the population** (as we saw earlier). In contrast, R calculates the **sample variance**.

The only difference between the **sample variance** is that instead of dividing by the total number of values at the end, it divides by the total number **minus one** (`4` instead of `5` in our previous case).

There's a perfectly good mathematical reason for this, and you can read more about it [here] (https://en.wikipedia.org/wiki/Variance#Population_variance_and_sample_variance). The only thing to bear in mind is that the larger the sample size, the more it makes no difference in practice.
